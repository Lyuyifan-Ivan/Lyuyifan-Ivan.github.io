# 激活函数那些事儿

- 神经网络为什么需要激活函数：首先数据的分布绝大多数是非线性的，而一般神经网络的计算是线性的，引入激活函数，是在神经网络中引入非线性，强化网络的学习能力。所以激活函数的最大特点就是非线性。

- 不同的激活函数，根据其特点，应用也不同。Sigmoid和tanh的特点是将输出限制在(0,1)和(-1,1)之间，说明Sigmoid和tanh适合做概率值的处理，例如LSTM中的各种门；而ReLU就不行，因为ReLU无最大值限制，可能会出现很大值。同样，根据ReLU的特征，Relu适合用于深层网络的训练，而Sigmoid和tanh则不行，因为它们会出现梯度消失。

## sigmoid函数
- sigmoid函数的取值范围在（0, 1）之间，可以将网络的输出映射在这一范围

### sigmoid公式及导数
$$\sigma(x)=\frac{1}{1+e^{-x}}$$
$$\sigma^{'}(x)=\frac{e^{-x}}{(1+e^{-x})^2}=(1-\sigma (x))\cdot \sigma(x)$$

<div align=center>
<img src="./常见问题汇总/Sigmoid.jpg"/>
</div>

sigmoid导数取值范围是[0, 0.25]，由于神经网络反向传播时的“链式反应”，很容易就会出现梯度消失的情况。例如对于一个10层的网络，根据$0.25^{10}\approx 0.000000954$，第10层的误差相对第一层卷积的参数$W_1$的梯度将是一个非常小的值，这就是所谓的“梯度消失”

Sigmoid的输出不是0均值（即zero-centered）；这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入，随着网络的加深，会改变数据的原始分布

## tanh函数

- tanh为双曲正切函数，tanh和sigmoid相似，都属于饱和激活函数，区别在于输出值范围由(0,1)变为了(-1,1)，可以把tanh函数看做是sigmoid向下平移和拉伸后的结果

### tanh公式及导数

$$tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}=\frac{2}{1+e^{-2x}}-1$$
可以更加清晰看出tanh与sigmoid函数的关系（平移+拉伸）
$$tanh^{'}(x)=1-tanh^2(x)$$

<div align=center>
<img src="./常见问题汇总/tanh.jpg"/>
</div>

- tanh的输出范围时(-1, 1)，解决了Sigmoid函数的不是zero-centered输出问题

- tanh导数范围在(0, 1)之间，相比sigmoid的(0, 0.25)，梯度消失（gradient vanishing）问题会得到缓解，但仍然还会存在

## ReLU函数

Relu(Rectified Linear Unit)——修正线性单元函数：该函数形式比较简单，其有效导数为常数1

$$relu(x)=max(0,x)$$

<div align=center>
<img src="./常见问题汇总/relu.jpg"/>
</div>

- 导数为常数1的好处就是在“链式反应”中不会出现梯度消失，但梯度下降的强度就完全取决于权值的乘积，这样就可能会出现梯度爆炸问题。解决这类问题：
  - 一是控制权值，让它们在（0，1）范围内
  - 二是做梯度裁剪，控制梯度下降强度，如ReLU(x)=min(6, max(0,x))

- ReLU 强制将x<0部分的输出置为0（置为0就是屏蔽该特征），可能会导致模型无法学习到有效特征，所以如果学习率设置的太大，就可能会导致网络的大部分神经元处于‘dead’状态，所以使用ReLU的网络，学习率不能设置太大